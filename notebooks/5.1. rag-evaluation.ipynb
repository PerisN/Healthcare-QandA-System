{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import minsearch2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON document with generated ids\n",
    "with open('..\\data\\data-with-ids.json', 'rt') as f_in:\n",
    "    documents = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model to use for embeddings\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch2.Index at 0x21b87b1d7e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_fields = ['question_answer']\n",
    "keyword_fields = ['id']\n",
    "\n",
    "# Create combined field in the documents\n",
    "for doc in documents:\n",
    "    doc['question_answer'] = doc['question'] + \" \" + doc['answer']\n",
    "\n",
    "# Create the index and fit the index\n",
    "index = minsearch2.Index(text_fields, keyword_fields)\n",
    "\n",
    "for doc in documents:\n",
    "    for field in text_fields:\n",
    "        doc[field] = model.encode(doc[field]).tolist()  # Convert numpy array to list\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(field, query_vector):\n",
    "    boost = {}\n",
    "    query = {field: query_vector.reshape(1, -1)}  # Reshape to 2D array\n",
    "    results = index.search(\n",
    "        query_vectors=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "    return results\n",
    "\n",
    "def minsearch_search_2(question):\n",
    "    field = 'question_answer'\n",
    "    \n",
    "    query_vector = model.encode([question])\n",
    "    return minsearch_search(field, query_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAG Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates a prompt for an LLM to answer health-related questions based on the given data\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a healthcare assistant AI. Answer the QUESTION based on the CONTEXT provided from a health FAQ database.\n",
    "Use only the facts from the CONTEXT to provide an accurate, clear, and concise answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context += f\"Question: {doc['question']}\\nAnswer: {doc['answer']}\\nSource: {doc['source']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, llm_model='gpt-4o-mini'):\n",
    "    \n",
    "    # Perform the search using the query vector\n",
    "    search_results = minsearch_search_2(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, model=llm_model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night blindness, also known as nyctalopia, can occur due to various reasons, including genetic conditions like X-linked congenital stationary night blindness. This specific condition is caused by mutations in the NYX and CACNA1F genes, which disrupt the function of photoreceptor cells (rods and cones) in the retina essential for low-light vision. When the rods, which are responsible for vision in low light, are severely affected, it results in night blindness. Additionally, other factors such as Vitamin A deficiency, retinal diseases, and various systemic diseases can also contribute to night blindness.\n"
     ]
    }
   ],
   "source": [
    "question = 'what causes night blindness?'\n",
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieval evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth data\n",
    "\n",
    "df_questions = pd.read_csv('..\\data\\ground-truth-retrieval.csv')\n",
    "ground_truth = df_questions.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8e76517a',\n",
       " 'question': 'What specific region on chromosome 2 is affected by 2q37 deletion syndrome?'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hit rate\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "# mrr\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "    \n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q['question'])\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "    \n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "636f0e7c1a324b2496e65a7e2c66d74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit_rate': 0.9924657534246575, 'mrr': 0.9561314959773861}\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate(ground_truth, minsearch_search_2)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAG Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_questions.sample(n=50, random_state=1)\n",
    "sample = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca6e8c6f23a465ca47a20b3274a4843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.88\n",
       "PARTLY_RELEVANT    0.06\n",
       "NON_RELEVANT       0.06\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Currently, there are no specific treatment opt...</td>\n",
       "      <td>c1abe560</td>\n",
       "      <td>What are the best treatment options available ...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not provide any info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The provided context does not include specific...</td>\n",
       "      <td>cd440cb8</td>\n",
       "      <td>How does Williams syndrome affect cognitive de...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not provide any info...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The provided context does not include specific...</td>\n",
       "      <td>8010eef7</td>\n",
       "      <td>Are there lifestyle choices that can help redu...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer        id  \\\n",
       "14  Currently, there are no specific treatment opt...  c1abe560   \n",
       "21  The provided context does not include specific...  cd440cb8   \n",
       "44  The provided context does not include specific...  8010eef7   \n",
       "\n",
       "                                             question     relevance  \\\n",
       "14  What are the best treatment options available ...  NON_RELEVANT   \n",
       "21  How does Williams syndrome affect cognitive de...  NON_RELEVANT   \n",
       "44  Are there lifestyle choices that can help redu...  NON_RELEVANT   \n",
       "\n",
       "                                          explanation  \n",
       "14  The generated answer does not provide any info...  \n",
       "21  The generated answer does not provide any info...  \n",
       "44  The generated answer explicitly states that it...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/rag-eval-gpt-4o-mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a590b1425bb7473da2d75c99df05b602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations_gpt_4o = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question, llm_model='gpt-4o') \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    evaluation = json.loads(evaluation)\n",
    "    \n",
    "    evaluations_gpt_4o.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval2 = pd.DataFrame(evaluations_gpt_4o, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval2['id'] = df_eval2.record.apply(lambda d: d['id'])\n",
    "df_eval2['question'] = df_eval2.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval2['relevance'] = df_eval2.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval2['explanation'] = df_eval2.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval2['record']\n",
    "del df_eval2['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.86\n",
       "PARTLY_RELEVANT    0.08\n",
       "NON_RELEVANT       0.06\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval2.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The context provided does not include specific...</td>\n",
       "      <td>c1abe560</td>\n",
       "      <td>What are the best treatment options available ...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The provided context does not contain specific...</td>\n",
       "      <td>cd440cb8</td>\n",
       "      <td>How does Williams syndrome affect cognitive de...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not provide any rele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The provided CONTEXT does not contain informat...</td>\n",
       "      <td>8010eef7</td>\n",
       "      <td>Are there lifestyle choices that can help redu...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer states that the provided ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer        id  \\\n",
       "14  The context provided does not include specific...  c1abe560   \n",
       "21  The provided context does not contain specific...  cd440cb8   \n",
       "44  The provided CONTEXT does not contain informat...  8010eef7   \n",
       "\n",
       "                                             question     relevance  \\\n",
       "14  What are the best treatment options available ...  NON_RELEVANT   \n",
       "21  How does Williams syndrome affect cognitive de...  NON_RELEVANT   \n",
       "44  Are there lifestyle choices that can help redu...  NON_RELEVANT   \n",
       "\n",
       "                                          explanation  \n",
       "14  The generated answer does not address the ques...  \n",
       "21  The generated answer does not provide any rele...  \n",
       "44  The generated answer states that the provided ...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval2[df_eval2.relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SMARCB1 and SMARCA4 gene testing for children ...</td>\n",
       "      <td>0a5de47f</td>\n",
       "      <td>How is the SMARCB1 and SMARCA4 gene testing co...</td>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer provides some relevant in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>To diagnose Deep Vein Thrombosis (DVT), your d...</td>\n",
       "      <td>c12d9fd1</td>\n",
       "      <td>What physical examination techniques will my d...</td>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer correctly mentions checki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Before a planned C-section, your doctor can pe...</td>\n",
       "      <td>703c3b5d</td>\n",
       "      <td>What tests can my doctor perform before a plan...</td>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer mentions that the doctor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Some of the tests used to detect childhood epe...</td>\n",
       "      <td>d2fb5d45</td>\n",
       "      <td>How often are tests repeated after surgery for...</td>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer provides some information...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer        id  \\\n",
       "2   SMARCB1 and SMARCA4 gene testing for children ...  0a5de47f   \n",
       "9   To diagnose Deep Vein Thrombosis (DVT), your d...  c12d9fd1   \n",
       "33  Before a planned C-section, your doctor can pe...  703c3b5d   \n",
       "35  Some of the tests used to detect childhood epe...  d2fb5d45   \n",
       "\n",
       "                                             question        relevance  \\\n",
       "2   How is the SMARCB1 and SMARCA4 gene testing co...  PARTLY_RELEVANT   \n",
       "9   What physical examination techniques will my d...  PARTLY_RELEVANT   \n",
       "33  What tests can my doctor perform before a plan...  PARTLY_RELEVANT   \n",
       "35  How often are tests repeated after surgery for...  PARTLY_RELEVANT   \n",
       "\n",
       "                                          explanation  \n",
       "2   The generated answer provides some relevant in...  \n",
       "9   The generated answer correctly mentions checki...  \n",
       "33  The generated answer mentions that the doctor ...  \n",
       "35  The generated answer provides some information...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval2[df_eval2.relevance == 'PARTLY_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval2.to_csv('../data/rag-eval-gpt-4o.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTP-4o-mini outperforms GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both models havethe same questions in the non-relevant then the answers have to be updated to provide context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
