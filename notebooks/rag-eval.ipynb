{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import minsearch\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON document with generated ids\n",
    "with open('..\\data\\data-with-ids.json', 'rt') as f_in:\n",
    "    documents = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x2cb8a8a1660>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index the documents\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=['question', 'answer', 'source', 'focus_area'],\n",
    "    keyword_fields=['id'])\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAG Flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search function\n",
    "\n",
    "def search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates a prompt for an LLM to answer health-related questions based on the given data\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a healthcare assistant AI. Answer the QUESTION based on the CONTEXT provided from a health FAQ database.\n",
    "Use only the facts from the CONTEXT to provide an accurate, clear, and concise answer.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context += f\"Question: {doc['question']}\\nAnswer: {doc['answer']}\\nSource: {doc['source']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query, model='gpt-4o-mini'):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Night blindness can be caused by mutations in the NYX and CACNA1F genes, which are linked to X-linked congenital stationary night blindness. These mutations disrupt the function of rods in the retina, which are essential for vision in low light conditions. When the rods are severely disrupted, night vision is impaired, leading to night blindness.\n"
     ]
    }
   ],
   "source": [
    "question = 'what cause night blindness?'\n",
    "answer = rag(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retrieval evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth data\n",
    "\n",
    "df_questions = pd.read_csv('..\\data\\ground-truth-retrieval.csv')\n",
    "ground_truth = df_questions.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '8e76517a',\n",
       " 'question': 'What specific region on chromosome 2 is affected by 2q37 deletion syndrome?'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hit rate\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "# mrr\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query):\n",
    "    boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f813852bcf4c41b882c9313721733d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9698630136986301, 'mrr': 0.9211912915851267}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: minsearch_search(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Finding the best parameters*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define validation and test sets\n",
    "\n",
    "df_validation = df_questions[:50]\n",
    "df_test = df_questions[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_optimize(param_ranges, objective_function, n_iterations=10):\n",
    "    best_params = None\n",
    "    best_score = float('-inf')  # Use float('-inf') if maximizing.\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # Generate random parameters\n",
    "        current_params = {}\n",
    "        for param, (min_val, max_val) in param_ranges.items():\n",
    "            if isinstance(min_val, int) and isinstance(max_val, int):\n",
    "                current_params[param] = random.randint(min_val, max_val)\n",
    "            else:\n",
    "                current_params[param] = random.uniform(min_val, max_val)\n",
    "        \n",
    "        # Evaluate the objective function\n",
    "        current_score = objective_function(current_params)\n",
    "        \n",
    "        # Update best if current is better\n",
    "        if current_score > best_score:\n",
    "            best_score = current_score\n",
    "            best_params = current_params\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_val = df_validation.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minsearch_search(query, boost=None):\n",
    "    if boost is None:\n",
    "        boost = {}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(boost_params):\n",
    "    def search_function(q):\n",
    "        return minsearch_search(q['question'], boost_params)\n",
    "\n",
    "    results = evaluate(gt_val, search_function)\n",
    "    return results['mrr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_ranges = {\n",
    "    'question': (0.0, 3.0),\n",
    "    'answer': (0.0, 3.0),\n",
    "    'source': (0.0, 3.0),\n",
    "    'focus_area': (0.0, 3.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a25a5553254585afbf56da4bb30967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967a3f09fad54d00bdc6418454e03b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c8e5db0ac34f68abba0e84c2f82cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4acb963f92745ceb2bc3b5396ecdf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098f02202f9b4fb28d139f878ec28f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b660842a9cb64b5eb0aaaa2fe52f8ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39a7761e88974b90afbd56c2a9daa37d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea8f2e7da7a439186ef41c2a2774bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9ea102fe5f44d78d862712b67af1de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4166fc8d8e04785bde1cbf16c9a031a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3222a9fcdd448d96e8bfc1fbc4f10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1de833e724aa41b0805313956d1f162c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572fcf3a9e294487aec0723e084ee690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce9a57956414e658b207d4a8f330402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b5edeec27c4c4fa3b743ecccc40e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7497250056941cc9155d302f3fe12e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9c27097ed74b958af6b7e80be64f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "745f604797144ec4b2caa369190662fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1797f6d2a2be436a800ef346f29ee079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265488fcfa894a928e219c0c7c389754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "({'question': 2.4933118294228587,\n",
       "  'answer': 2.602597231352311,\n",
       "  'source': 2.848426640576907,\n",
       "  'focus_area': 0.8874827702033717},\n",
       " 0.99)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_optimize(param_ranges, objective, n_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02848aa1bafc433e8332e36301e188e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9801369863013699, 'mrr': 0.9329280821917806}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minsearch_improved(query):\n",
    "    boost = {\n",
    "        'question': 2.4933,\n",
    "        'answer': 2.6025,\n",
    "        'source': 2.8484,\n",
    "        'focus_area': 0.8874\n",
    "    }\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={},\n",
    "        boost_dict=boost,\n",
    "        num_results=10\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "evaluate(ground_truth, lambda q: minsearch_improved(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RAG Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2_template = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_questions.sample(n=50, random_state=1)\n",
    "sample = df_sample.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760ba5117a1c41a9a00b81afa504cac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question) \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    evaluation = json.loads(evaluation)\n",
    "\n",
    "    evaluations.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.84\n",
       "NON_RELEVANT       0.10\n",
       "PARTLY_RELEVANT    0.06\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The provided context does not include informat...</td>\n",
       "      <td>0a5de47f</td>\n",
       "      <td>How is the SMARCB1 and SMARCA4 gene testing co...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The provided context does not include any spec...</td>\n",
       "      <td>c1abe560</td>\n",
       "      <td>What are the best treatment options available ...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The context provided does not include specific...</td>\n",
       "      <td>cd440cb8</td>\n",
       "      <td>How does Williams syndrome affect cognitive de...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Your provided context does not contain informa...</td>\n",
       "      <td>703c3b5d</td>\n",
       "      <td>What tests can my doctor perform before a plan...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Based on the provided context, there is no spe...</td>\n",
       "      <td>8010eef7</td>\n",
       "      <td>Are there lifestyle choices that can help redu...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer explicitly states that th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer        id  \\\n",
       "2   The provided context does not include informat...  0a5de47f   \n",
       "14  The provided context does not include any spec...  c1abe560   \n",
       "21  The context provided does not include specific...  cd440cb8   \n",
       "33  Your provided context does not contain informa...  703c3b5d   \n",
       "44  Based on the provided context, there is no spe...  8010eef7   \n",
       "\n",
       "                                             question     relevance  \\\n",
       "2   How is the SMARCB1 and SMARCA4 gene testing co...  NON_RELEVANT   \n",
       "14  What are the best treatment options available ...  NON_RELEVANT   \n",
       "21  How does Williams syndrome affect cognitive de...  NON_RELEVANT   \n",
       "33  What tests can my doctor perform before a plan...  NON_RELEVANT   \n",
       "44  Are there lifestyle choices that can help redu...  NON_RELEVANT   \n",
       "\n",
       "                                          explanation  \n",
       "2   The generated answer explicitly states that it...  \n",
       "14  The generated answer does not address the ques...  \n",
       "21  The generated answer explicitly states that it...  \n",
       "33  The generated answer does not address the ques...  \n",
       "44  The generated answer explicitly states that th...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/rag-eval-gpt-4o-mini.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60a98668b274d1b8f104852ba7f6e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluations_gpt4o = []\n",
    "\n",
    "for record in tqdm(sample):\n",
    "    question = record['question']\n",
    "    answer_llm = rag(question, model='gpt-4o') \n",
    "\n",
    "    prompt = prompt2_template.format(\n",
    "        question=question,\n",
    "        answer_llm=answer_llm\n",
    "    )\n",
    "\n",
    "    evaluation = llm(prompt)\n",
    "    evaluation = json.loads(evaluation)\n",
    "    \n",
    "    evaluations_gpt4o.append((record, answer_llm, evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.DataFrame(evaluations_gpt4o, columns=['record', 'answer', 'evaluation'])\n",
    "\n",
    "df_eval['id'] = df_eval.record.apply(lambda d: d['id'])\n",
    "df_eval['question'] = df_eval.record.apply(lambda d: d['question'])\n",
    "\n",
    "df_eval['relevance'] = df_eval.evaluation.apply(lambda d: d['Relevance'])\n",
    "df_eval['explanation'] = df_eval.evaluation.apply(lambda d: d['Explanation'])\n",
    "\n",
    "del df_eval['record']\n",
    "del df_eval['evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "RELEVANT           0.88\n",
       "NON_RELEVANT       0.10\n",
       "PARTLY_RELEVANT    0.02\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.relevance.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The provided context does not include any info...</td>\n",
       "      <td>0a5de47f</td>\n",
       "      <td>How is the SMARCB1 and SMARCA4 gene testing co...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer states that there is no i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The context provided does not give specific in...</td>\n",
       "      <td>c1abe560</td>\n",
       "      <td>What are the best treatment options available ...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The provided context does not contain specific...</td>\n",
       "      <td>cd440cb8</td>\n",
       "      <td>How does Williams syndrome affect cognitive de...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer states that it cannot pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The provided context does not contain any info...</td>\n",
       "      <td>703c3b5d</td>\n",
       "      <td>What tests can my doctor perform before a plan...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer acknowledges a lack of in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The provided context does not include specific...</td>\n",
       "      <td>8010eef7</td>\n",
       "      <td>Are there lifestyle choices that can help redu...</td>\n",
       "      <td>NON_RELEVANT</td>\n",
       "      <td>The generated answer does not address the ques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer        id  \\\n",
       "2   The provided context does not include any info...  0a5de47f   \n",
       "14  The context provided does not give specific in...  c1abe560   \n",
       "21  The provided context does not contain specific...  cd440cb8   \n",
       "33  The provided context does not contain any info...  703c3b5d   \n",
       "44  The provided context does not include specific...  8010eef7   \n",
       "\n",
       "                                             question     relevance  \\\n",
       "2   How is the SMARCB1 and SMARCA4 gene testing co...  NON_RELEVANT   \n",
       "14  What are the best treatment options available ...  NON_RELEVANT   \n",
       "21  How does Williams syndrome affect cognitive de...  NON_RELEVANT   \n",
       "33  What tests can my doctor perform before a plan...  NON_RELEVANT   \n",
       "44  Are there lifestyle choices that can help redu...  NON_RELEVANT   \n",
       "\n",
       "                                          explanation  \n",
       "2   The generated answer states that there is no i...  \n",
       "14  The generated answer does not address the ques...  \n",
       "21  The generated answer states that it cannot pro...  \n",
       "33  The generated answer acknowledges a lack of in...  \n",
       "44  The generated answer does not address the ques...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.relevance == 'NON_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>relevance</th>\n",
       "      <th>explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Erdheim-Chester disease is not inherited and i...</td>\n",
       "      <td>2c6dacab</td>\n",
       "      <td>Are there any genetic factors associated with ...</td>\n",
       "      <td>PARTLY_RELEVANT</td>\n",
       "      <td>The generated answer indicates that Erdheim-Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               answer        id  \\\n",
       "10  Erdheim-Chester disease is not inherited and i...  2c6dacab   \n",
       "\n",
       "                                             question        relevance  \\\n",
       "10  Are there any genetic factors associated with ...  PARTLY_RELEVANT   \n",
       "\n",
       "                                          explanation  \n",
       "10  The generated answer indicates that Erdheim-Ch...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval[df_eval.relevance == 'PARTLY_RELEVANT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('../data/rag-eval-gpt-4o.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
